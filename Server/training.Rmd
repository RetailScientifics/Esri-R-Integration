---
title: "Model Training"
output:
  html_document:
    fig_caption: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
---

The purpose of this notebook is to train a toy model to demonstrate how R modeling can be tied in with web-based frontends. In this case, we will use a shapefile that contains latitudes, longitudes, and a number of demography variables for polygons around the US, we'll mock up a response variable that represents revenue within each district, and then create a model in R to predict revenue in new locations.

The model we'll be fitting is a simple [linear regression](http://r-statistics.co/Linear-Regression.html) -- this model may or may not have predictive power in this case, but is perhaps the simplest type of model to set up and use.

# Setup

## Libraries
The code below will attempt to require the given list of libraries, and automatically install any that are missing.
```{r warning==FALSE}
set.seed(1)
sapply(c(
	'plumber',
	'tidyverse',
	'caret',
	'rgdal',
	'rgeos',
	'readr',
	'GetoptLong'
), function(p) {
	if (!requireNamespace(p, quietly = TRUE)) {
		install.packages(p, quiet = TRUE)
	}
	require(p, character.only = TRUE, quietly = TRUE)
})
```

## Reading in a Shapefile

For the purposes of this example, we'll use the '2016 Population Density by Congressional District' shapefile provided by the Esri Demographics team, which can be found at [https://www.arcgis.com/home/item.html?id=ff48bbae433442a38f6c635b8c7baf72](https://www.arcgis.com/home/item.html?id=ff48bbae433442a38f6c635b8c7baf72).

We've downloaded and unzipped it into the 'files' subdirectory, so we can now read it into R using either the rgdal library. If you have Esris' [Arcgis r-bridge package](https://github.com/R-ArcGIS/r-bridge) installed, you can read the file in analogously (plus much more!) or even import it directly from ArcGIS Online without downloading/unzipping.
```{r}
shapefile <- rgdal::readOGR(
	dsn = "files/2016_Population_Density_by_Congressional_District.shp",
	stringsAsFactors = FALSE
)

# Note: if you have the arcgisbinding package installed, you can alternatively just run the following:
#
# shapefile <- arcgisbinding::arc.data2sp(arc.select(arc.open(
#   path = "files/2016_Population_Density_by_Congressional_District.shp"))
# )
#
# To import directly from ArcGIS Online, you can use this instead:
#
# shapefile <- arcgisbinding::arc.data2sp(
#   arc.select(
#     arc.open(
#       path = "https://services.arcgis.com/P3ePLMYs2RVChkJx/ArcGIS/rest/services/Congressional_District_Demographics/FeatureServer/0"
#     )
#   )
# )


shapefile@data <- shapefile@data %>%
	mutate_at(
	  vars(TOTPOP_CY:GenSilent), as.numeric
	)

stagedShapefile <- SpatialPointsDataFrame(gCentroid(shapefile, byid=TRUE), shapefile@data) %>%
  as.tibble() %>%
  select( colnames(.) %>% order ) %>%
  select( -OBJECTID, -ID, -NAME, -ST_ABBREV ) %>%
  select( x, y, everything() )

head(stagedShapefile)
```

Next, we'll add a "Revenue" column that will serve as our response variable, with an average of about $100,000. We'll then look at some summary statistics to get an idea of how this simulated revenue is distributed:
```{r fig.height = 6, fig.width = 20, fig.align = "center"}
set.seed(123)
stagedShapefile$Revenue <- rnorm(n = nrow(stagedShapefile), mean = 100000, sd = 10000)

summary( stagedShapefile$Revenue )

ggplot( stagedShapefile ) +
  geom_boxplot(
    aes(y = Revenue), notch = TRUE,
    fill = 'blue', alpha = 0.3,
    outlier.colour = "#1F3552", outlier.shape = 20
  ) +
  coord_flip() +
  theme_grey()

ggplot( stagedShapefile, aes(x = Revenue) ) +
  geom_histogram( aes(y=..density..), bins = 100, fill = 'blue', alpha = 0.2 ) +
  geom_density( aes(y=..density..), fill = 'green', alpha = 0.2 )

```

# Modeling

## Training

Before training the model, we will need to perform some minor preprocessing on the input data.

The first preprocessing step we'll use is to remove columns that are linearly dependent - that is, columns that can be obtained by adding or subtracting multiples of other columns, making them redundant.
```{r}
dfClean <- stagedShapefile[, -caret::findLinearCombos(stagedShapefile)$remove]
print(GetoptLong::qq("Linearly dependent columns removed: @{ncol(stagedShapefile) - ncol(dfClean)}"))
```

From this cleaned data set, we'll create some derived variables from the remaining columns, while dropping the columns they originally came from. This allows us to simulate categorical variable. This technique can also be used when the exact numerical value of a regressor isn't especially important, but one still wants to incorporate date concerning roughly where it falls within the original distribution.
```{r}
dfClean2 <- dfClean %>%
  # Binning population density into 5 categories
  mutate(
    PopulationDensity = cut(
      POPDENS_CY,
      breaks = quantile( .$POPDENS_CY, 0:5 * (1/5) ),
      labels = c( "low", "medium-low", "medium", "medium-high", "high" )
    )
  ) %>%
  # Binning percentage of baby boomers into 3 categories
  mutate(
    PropBoomers = GenBoom / stagedShapefile$TOTPOP_CY
  ) %>%
  mutate(
    PropBoomers = cut(
      PropBoomers,
      breaks = quantile( .$PropBoomers, 0:3 * (1/3) ),
      labels = c( "low", "average", "high" )
    )
  ) %>%
  # Setting 3 flags for when demography variables are above average
  mutate(
    HighlyEducated = ifelse(GRADDEG_CY >= mean(stagedShapefile$GRADDEG_CY), TRUE, FALSE),
    ManyWidows = ifelse(WIDOWED_CY >= mean(stagedShapefile$WIDOWED_CY), TRUE, FALSE),
    LargePopulation = ifelse(HHPOP_CY >= mean(stagedShapefile$HHPOP_CY), TRUE, FALSE)
  ) %>%
  # Dropping original columns
  select(-POPDENS_CY, -GenBoom, -GRADDEG_CY, -WIDOWED_CY, -HHPOP_CY) %>%
  # Rearrange columns to see relevant variables in output
  select(
    x, y, Revenue, PopulationDensity, PropBoomers, HighlyEducated, ManyWidows, LargePopulation, everything()
  ) %>%
  # Using quantiles may introduce 1 NA per variable at the 0% quantile, to simplify we just drop this observation
  drop_na

head(dfClean2)
```


Next, we'll split the full dataset into a training set (90% of the data) and a holdout or testing set (the remaining 10%). This will allow us to assess the model's accuracy by attempting to make predictions on the unseen testing set, for which we know the actual response variable and can use the difference as a measure of error.
```{r}
trainIndices <- createDataPartition(
  dfClean2 %>% pull(Revenue),
  p = 0.90,
  list = FALSE,
  times = 1
)

training <- dfClean2[ trainIndices,]
test  <- dfClean2[-trainIndices,]
```

We'll now fit a linear model to the training set. Here, we're using the `train` function from caret, which allows specifying more preprocessing steps to be applied to the training data (which will also automatically be applied to any data supplied for future predictions).

The first argument is a formula -- in this case, we are specifying `Revenue` as a function of all remaining variables (denoted by `.`).

For preprocessing, we are centering and scaling the data so it has a mean of zero and a standard deviation of one unit. We also remove columns with near-zero variance (nzv), i.e. those which tend to be nearly constant across all of the training data. Finally, we apply a Yeo-Johnson transformation to stabilize the variance of the input data, making it more closely resemble a normal distribution.

After training the linear model, we can examine the coefficients of each variable in the regression.

```{r}
lmFit <- caret::train(
  Revenue ~ . ,
  data = training,
  method = "lm"
)
print(lmFit$finalModel$coefficients)
```

We now have a model that takes in a latitude, longitude, several derived variables, as well as a host of demography variables sourced from the shapefile, and attempts to predict the unknown revenue for that location based on all of this information.

## Performance / Error

We can measure the error in two ways - the training error and the test error.

The training error indicates how well the model predicts data it has already seen -- or in other words, how well the model fits the training data. The testing error indicates how well the model might predict on new, unseen data, and is generally the indicator one is most concerned with.

We'll write a function to print a number of summary statistics, described below
```{r}
printStats <- function(y_true, y_pred) {
  print(GetoptLong::qq("MAE: @{MLmetrics::MAE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("RMAE: @{MLmetrics::MAE(y_pred = y_pred, y_true = y_true) / mean(y_true) %>% round(4)}"))
  print(GetoptLong::qq("MAPE: @{MLmetrics::MAPE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("RMSE: @{MLmetrics::RMSE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("R^2: @{MLmetrics::R2_Score(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("Correlation: @{cor(y_pred, y_true)}"))
}
```

The error metrics used are:

* MAE: [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error)
* RMAE: Relative Mean Absolute Error, i.e. the MAE divided by the average value of the training set
* MAPE: [Mean Absolute Percent Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)
* $R^2$: [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)
* [Correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence#Pearson's_product-moment_coefficient)

### Training Error

This helps answer the question, "How well does this model perform on data it has already seen?" 

Here we expect error to be low -- but not perfect, as zero error would indicate [overfitting](https://en.wikipedia.org/wiki/Overfitting), leading to high variance and unrealistic predictions.
```{r}
printStats(
  y_pred = predict(lmFit, training %>% select(-Revenue)),
  y_true = training %>% pull(Revenue)
)
```

### Testing Error
We now ask the question, "How well might this model perform on new, unseen data?" 

In this case, we can optimistically hope for the error to be as low as possible.
```{r}
printStats(
  y_pred = predict(lmFit, test %>% select(-Revenue)),
  y_true = test %>% pull(Revenue)
)
```

So by examining the MAPE (for example), we find that our training error is roughly `r (MLmetrics::MAPE(y_pred = predict(lmFit, training %>% select(-Revenue)), y_true = training %>% pull(Revenue)) * 100) %>% round(2)`% while the error on the testing set is about `r (MLmetrics::MAPE(y_pred = predict(lmFit, test %>% select(-Revenue)), y_true = test %>% pull(Revenue)) * 100) %>% round(2)`%.

## Saving

With a model in hand, can now save it out to file, which we can load at any time to stage new data and run predictions.
```{r}
readr::write_rds(lmFit, "files/linear_model.rds")
print('Model written to file')
```
